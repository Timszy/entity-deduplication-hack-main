{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581f2770",
   "metadata": {},
   "source": [
    "## Check for Programmatic variations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50605b8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'matches/SentRevised_filtered.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m      5\u001b[0m golden_standard \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/prog_data/updated_golden_standard_duplicates2.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load both match files\u001b[39;00m\n\u001b[0;32m      8\u001b[0m match_files \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHacky\u001b[39m\u001b[38;5;124m'\u001b[39m: json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatches/matchesHacky.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m#'Sent_revised': json.load(open('matches/Sent_revised.json', 'r')),\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistmult\u001b[39m\u001b[38;5;124m'\u001b[39m: json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatches/HybridDistmult_filtered.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNode2vec\u001b[39m\u001b[38;5;124m'\u001b[39m: json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatches/HybridNode2vec_filtered.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#'Sent_revised_g2': json.load(open('matches/SentRevisedg2.json', 'r')),\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m#'Sent_revised_g3': json.load(open('matches/SentRevisedv2.json', 'r')),\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSent_filtered\u001b[39m\u001b[38;5;124m'\u001b[39m: json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmatches/SentRevised_filtered.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m),\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m#'Sent_revised_g4': json.load(open('matches/SentRevisedv3.json', 'r')),  \u001b[39;00m\n\u001b[0;32m     17\u001b[0m }\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_uuid\u001b[39m(uri):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uri\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32md:\\Program Files\\Uni\\Master Scriptie\\Code Snippets\\entity-deduplication-hack-main\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'matches/SentRevised_filtered.json'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV and JSON files\n",
    "golden_standard = pd.read_csv('data/prog_data/updated_golden_standard_duplicates2.csv')\n",
    "\n",
    "# Load both match files\n",
    "match_files = {\n",
    "    'Hacky': json.load(open('matches/matchesHacky.json', 'r')),\n",
    "    #'Sent_revised': json.load(open('matches/Sent_revised.json', 'r')),\n",
    "    'Distmult': json.load(open('matches/HybridDistmult_filtered.json', 'r')),\n",
    "    'Node2vec': json.load(open('matches/HybridNode2vec_filtered.json', 'r')),\n",
    "    'Sent_filtered': json.load(open('matches/SentenceEmbedding_filtered.json', 'r')),\n",
    "}\n",
    "\n",
    "def extract_uuid(uri):\n",
    "    return uri.split(\"/\")[-1]\n",
    "\n",
    "# Process each match file\n",
    "for match_type, data in match_files.items():\n",
    "    identifiers = []\n",
    "\n",
    "    # Extract UUIDs directly from the subject URIs\n",
    "    for match in data:\n",
    "        pair = {}\n",
    "        for entity_label in ['entity1', 'entity2']:\n",
    "            entity = next(e[entity_label] for e in match['entities'] if entity_label in e)\n",
    "            uri = entity.get(\"subject\")\n",
    "            pair[entity_label] = extract_uuid(uri) if uri else None\n",
    "        identifiers.append(pair)\n",
    "\n",
    "    # Check matches against the golden standard\n",
    "    matched_rows = golden_standard[\n",
    "        golden_standard.apply(\n",
    "            lambda row: any(\n",
    "                (pair['entity1'] == row['original_id'] and pair['entity2'] == row['duplicate_id']) \n",
    "                or (pair['entity1'] == row['duplicate_id'] and pair['entity2'] == row['original_id'])  # bi-directional match\n",
    "                for pair in identifiers\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Overall matching statistics\n",
    "    total = len(golden_standard)\n",
    "    matched = len(matched_rows)\n",
    "    pct_matched = matched / total * 100\n",
    "\n",
    "    print(f\"Matching Statistics for {match_type}:\")\n",
    "    print(f\"Number of matches in {match_type}:\", len(match_files[match_type]))\n",
    "    print(f\"Total Duplicates in Golden Standard: {total}\")\n",
    "    print(f\"Total Matched Duplicates: {matched}\")\n",
    "    print(f\"Percentage Matched (%): {pct_matched:.2f}\\n\")\n",
    "\n",
    "    # Variation-type analysis\n",
    "    gold_var = golden_standard['variation_type'].value_counts().rename('Golden Standard Count')\n",
    "    match_var = matched_rows['variation_type'].value_counts().rename('Matched Count')\n",
    "\n",
    "    variation_df = pd.concat([gold_var, match_var], axis=1).fillna(0).astype(int)\n",
    "    variation_df['Matched (%)'] = variation_df['Matched Count'] / variation_df['Golden Standard Count'] * 100\n",
    "    variation_df = variation_df.sort_index()\n",
    "\n",
    "    \n",
    "\n",
    "    # Entity-type analysis\n",
    "    gold_ent = golden_standard['entity_type'].value_counts().rename('Golden Standard Count')\n",
    "    match_ent = matched_rows['entity_type'].value_counts().rename('Matched Count')\n",
    "\n",
    "    entity_df = pd.concat([gold_ent, match_ent], axis=1).fillna(0).astype(int)\n",
    "    entity_df['Matched (%)'] = entity_df['Matched Count'] / entity_df['Golden Standard Count'] * 100\n",
    "    entity_df = entity_df.sort_index()\n",
    "\n",
    "    \n",
    "\n",
    "    # Combined variation and entity-type analysis\n",
    "    variation_entity_df = golden_standard.groupby(['variation_type', 'entity_type']).size().unstack(fill_value=0)\n",
    "    matched_variation_entity_df = matched_rows.groupby(['variation_type', 'entity_type']).size().unstack(fill_value=0)\n",
    "\n",
    "    frames = []\n",
    "    for vtype in variation_entity_df.index:\n",
    "        for etype in variation_entity_df.columns:\n",
    "            golden_count = variation_entity_df.at[vtype, etype] if etype in variation_entity_df.columns else 0\n",
    "            matched_count = matched_variation_entity_df.at[vtype, etype] if (vtype in matched_variation_entity_df.index and etype in matched_variation_entity_df.columns) else 0\n",
    "            matched_pct = (matched_count / golden_count * 100) if golden_count > 0 else 0\n",
    "            frames.append({\n",
    "                'variation_type': vtype,\n",
    "                'entity_type': etype,\n",
    "                'Golden Standard Count': golden_count,\n",
    "                'Matched Count': matched_count,\n",
    "                'Matched (%)': matched_pct\n",
    "            })\n",
    "\n",
    "    variation_entity_frame = pd.DataFrame(frames)\n",
    "    variation_entity_frame = variation_entity_frame.sort_values(['variation_type', 'entity_type']).reset_index(drop=True)\n",
    "    variation_frame = variation_entity_frame[variation_entity_frame['Golden Standard Count'] > 1]\n",
    "    variation_frame = variation_frame.sort_values('entity_type').reset_index(drop=True)\n",
    "    variation_frame\n",
    "\n",
    " # Set pandas display options to use the full width of the notebook\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.expand_frame_repr', False)\n",
    "    print(f\"Combined Variation and Entity-Type Analysis for {match_type}:\")\n",
    "    print(variation_frame)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cabd1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16de86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches in Dist: 1506\n",
      "Number of matches in DistLit: 12\n",
      "[{'entity1_id': 'e775f27e-2ec0-42f0-9310-0ac51932263c', 'entity2_id': 'ccc9ccfc-8111-449f-97ea-568c40b0aa75', 'score': 0.5635482668876648}, {'entity1_id': 'a88a96ca-4a8c-4f0a-a9e5-7340fdc60cf4', 'entity2_id': 'd3d90822-0729-4e06-a591-03107ef3782b', 'score': 0.5461329221725464}, {'entity1_id': 'beb2d716-50a2-4f8e-b7ed-8fd685a46efd', 'entity2_id': '3543d36c-da74-4ac4-a443-530c30e5d112', 'score': 0.5020403861999512}, {'entity1_id': '0bd293e3-ee6e-4267-a278-aafc9c62e74b', 'entity2_id': 'cff8cf24-ddc0-4140-b4e9-bfc61b416d8f', 'score': 0.9183053970336914}, {'entity1_id': '8f76781e-2215-44aa-9e5c-fe61f1e5023e', 'entity2_id': '6573d6cc-8f64-4754-bf48-7d8dca63cccc', 'score': 0.7842326164245605}]\n",
      "Number of matched pairs in Dist: 0 / 789 Percentage Matched (%): 0.00\n",
      "[{'entity1_id': '86e280c0-6036-4f9c-a229-7cb2cf00f0a8', 'entity2_id': 'f0af5825-66e5-4e50-8c24-8b75c635a410', 'score': 0.5034458637237549}, {'entity1_id': '10d44b3d-8aca-4410-9c10-126197762b3a', 'entity2_id': '5536945d-f70d-4f38-9d1d-3ec446e93aac', 'score': 0.507127046585083}, {'entity1_id': '4417d108-e256-4d3a-a90c-85a21ac2a40f', 'entity2_id': '7ca2f1ab-3308-4746-a480-5be48664f6a7', 'score': 0.5050807595252991}, {'entity1_id': '5d637363-f92e-4675-a1ae-a2e22ebdb2dc', 'entity2_id': 'e511c128-d619-4c01-accb-998ea28d2b0c', 'score': 0.5135082006454468}, {'entity1_id': '6eb8f2f2-6968-4ce4-8645-11ec8f14e2f9', 'entity2_id': '66b827d7-e4bc-419f-ae22-eab0a1142376', 'score': 0.5033048391342163}]\n",
      "Number of matched pairs in DistLit: 0 / 789 Percentage Matched (%): 0.00\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load your match files\n",
    "match_files_oldform = {\n",
    "    'Dist': json.load(open('Distmatches.json', 'r')),\n",
    "    'DistLit': json.load(open('DistLitmatches.json', 'r'))\n",
    "}\n",
    "\n",
    "print(\"Number of matches in Dist:\", len(match_files_oldform['Dist']))\n",
    "print(\"Number of matches in DistLit:\", len(match_files_oldform['DistLit']))\n",
    "\n",
    "# Loop through the correct dictionary\n",
    "for match_type, data in match_files_oldform.items():\n",
    "    extracted_pairs = []\n",
    "\n",
    "    for item in data:\n",
    "        # Defensive check to ensure keys exist\n",
    "        if all(k in item for k in (\"entity1\", \"entity2\", \"score\")):\n",
    "            extracted_pairs.append({\n",
    "                \"entity1_id\": item[\"entity1\"].rsplit(\"/\", 1)[-1],\n",
    "                \"entity2_id\": item[\"entity2\"].rsplit(\"/\", 1)[-1],\n",
    "                \"score\": item[\"score\"]\n",
    "            })\n",
    "        \n",
    "            \n",
    "    print(extracted_pairs[:5])  # Print first 5 pairs for verification\n",
    "    # Check matches against the golden standard\n",
    "    matched_rows = golden_standard[\n",
    "        golden_standard.apply(\n",
    "            lambda row: any(\n",
    "                (pair['entity1_id'] == row['original_id'] and pair['entity2_id'] == row['duplicate_id']) \n",
    "                for pair in identifiers\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "    ]\n",
    "    pct_matched = len(matched_rows) / len(golden_standard) * 100 if len(golden_standard) > 0 else 0\n",
    "    # Print the number of matched pairs for the current file\n",
    "    print(f\"Number of matched pairs in {match_type}: {len(matched_rows)} / {len(golden_standard)} Percentage Matched (%): {pct_matched:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
