{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581f2770",
   "metadata": {},
   "source": [
    "## Check for Programmatic variations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50605b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Statistics for Distmult:\n",
      "Number of matches in Distmult: 800\n",
      "Total Duplicates in Golden Standard: 789\n",
      "Total Matched Duplicates: 637\n",
      "Percentage Matched (%): 80.74\n",
      "\n",
      "Combined Variation and Entity-Type Analysis for Distmult:\n",
      "             variation_type             entity_type  Golden Standard Count  Matched Count  Matched (%)\n",
      "0                 city_typo                 Address                     20             20   100.000000\n",
      "1         country_expansion                 Address                     16              3    18.750000\n",
      "2             postal_format                 Address                     26             26   100.000000\n",
      "3       house_number_suffix                 Address                     27             27   100.000000\n",
      "4       email_domain_change            ContactPoint                     47             46    97.872340\n",
      "5                email_typo            ContactPoint                     42             40    95.238095\n",
      "6                 name_typo  HealthcareOrganization                      7              3    42.857143\n",
      "7         name_abbreviation  HealthcareOrganization                      3              1    33.333333\n",
      "8                email_typo     HealthcarePersonnel                    261            261   100.000000\n",
      "9                 name_typo                  Person                     51             51   100.000000\n",
      "10                name_swap                  Person                     48             48   100.000000\n",
      "11   abbreviated_first_name                  Person                     53             52    98.113208\n",
      "12    date_format_variation                  Person                     53             52    98.113208\n",
      "13       language_expansion                  Person                     56              3     5.357143\n",
      "14  department_abbreviation       ServiceDepartment                     23              1     4.347826\n",
      "15       alternative_naming       ServiceDepartment                     28              0     0.000000\n",
      "16              translation       ServiceDepartment                     28              3    10.714286\n",
      "Matching Statistics for NOde2vec:\n",
      "Number of matches in NOde2vec: 804\n",
      "Total Duplicates in Golden Standard: 789\n",
      "Total Matched Duplicates: 634\n",
      "Percentage Matched (%): 80.35\n",
      "\n",
      "Combined Variation and Entity-Type Analysis for NOde2vec:\n",
      "             variation_type             entity_type  Golden Standard Count  Matched Count  Matched (%)\n",
      "0                 city_typo                 Address                     20             20   100.000000\n",
      "1         country_expansion                 Address                     16              3    18.750000\n",
      "2             postal_format                 Address                     26             26   100.000000\n",
      "3       house_number_suffix                 Address                     27             27   100.000000\n",
      "4       email_domain_change            ContactPoint                     47             46    97.872340\n",
      "5                email_typo            ContactPoint                     42             42   100.000000\n",
      "6                 name_typo  HealthcareOrganization                      7              2    28.571429\n",
      "7         name_abbreviation  HealthcareOrganization                      3              1    33.333333\n",
      "8                email_typo     HealthcarePersonnel                    261            261   100.000000\n",
      "9                 name_typo                  Person                     51             51   100.000000\n",
      "10                name_swap                  Person                     48             48   100.000000\n",
      "11   abbreviated_first_name                  Person                     53             52    98.113208\n",
      "12    date_format_variation                  Person                     53             52    98.113208\n",
      "13       language_expansion                  Person                     56              3     5.357143\n",
      "14  department_abbreviation       ServiceDepartment                     23              0     0.000000\n",
      "15       alternative_naming       ServiceDepartment                     28              0     0.000000\n",
      "16              translation       ServiceDepartment                     28              0     0.000000\n",
      "Matching Statistics for Sent_filtered:\n",
      "Number of matches in Sent_filtered: 784\n",
      "Total Duplicates in Golden Standard: 789\n",
      "Total Matched Duplicates: 622\n",
      "Percentage Matched (%): 78.83\n",
      "\n",
      "Combined Variation and Entity-Type Analysis for Sent_filtered:\n",
      "             variation_type             entity_type  Golden Standard Count  Matched Count  Matched (%)\n",
      "0                 city_typo                 Address                     20             20   100.000000\n",
      "1         country_expansion                 Address                     16              2    12.500000\n",
      "2             postal_format                 Address                     26             26   100.000000\n",
      "3       house_number_suffix                 Address                     27             27   100.000000\n",
      "4       email_domain_change            ContactPoint                     47             44    93.617021\n",
      "5                email_typo            ContactPoint                     42             28    66.666667\n",
      "6                 name_typo  HealthcareOrganization                      7              3    42.857143\n",
      "7         name_abbreviation  HealthcareOrganization                      3              1    33.333333\n",
      "8                email_typo     HealthcarePersonnel                    261            261   100.000000\n",
      "9                 name_typo                  Person                     51             49    96.078431\n",
      "10                name_swap                  Person                     48             48   100.000000\n",
      "11   abbreviated_first_name                  Person                     53             52    98.113208\n",
      "12    date_format_variation                  Person                     53             52    98.113208\n",
      "13       language_expansion                  Person                     56              3     5.357143\n",
      "14  department_abbreviation       ServiceDepartment                     23              2     8.695652\n",
      "15       alternative_naming       ServiceDepartment                     28              0     0.000000\n",
      "16              translation       ServiceDepartment                     28              4    14.285714\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV and JSON files\n",
    "golden_standard = pd.read_csv('data/prog_data/updated_golden_standard_duplicates.csv')\n",
    "\n",
    "# Load both match files\n",
    "match_files = {\n",
    "    #'Hacky': json.load(open('matches/matchesHacky.json', 'r')),\n",
    "    #'Sent_revised': json.load(open('matches/Sent_revised.json', 'r')),\n",
    "    'Distmult': json.load(open('matches/Distmult_filtered.json', 'r')),\n",
    "    'NOde2vec': json.load(open('matches/Node2vec_filtered.json', 'r')),\n",
    "    #'Sent_revised_g2': json.load(open('matches/SentRevisedg2.json', 'r')),\n",
    "    #'Sent_revised_g3': json.load(open('matches/SentRevisedv2.json', 'r')),\n",
    "    'Sent_filtered': json.load(open('matches/SentRevised_filtered.json', 'r')),\n",
    "    #'Sent_revised_g4': json.load(open('matches/SentRevisedv3.json', 'r')),  \n",
    "}\n",
    "\n",
    "def extract_uuid(uri):\n",
    "    return uri.split(\"/\")[-1]\n",
    "\n",
    "# Process each match file\n",
    "for match_type, data in match_files.items():\n",
    "    identifiers = []\n",
    "\n",
    "    # Extract UUIDs directly from the subject URIs\n",
    "    for match in data:\n",
    "        pair = {}\n",
    "        for entity_label in ['entity1', 'entity2']:\n",
    "            entity = next(e[entity_label] for e in match['entities'] if entity_label in e)\n",
    "            uri = entity.get(\"subject\")\n",
    "            pair[entity_label] = extract_uuid(uri) if uri else None\n",
    "        identifiers.append(pair)\n",
    "\n",
    "    # Check matches against the golden standard\n",
    "    matched_rows = golden_standard[\n",
    "        golden_standard.apply(\n",
    "            lambda row: any(\n",
    "                (pair['entity1'] == row['original_id'] and pair['entity2'] == row['duplicate_id']) \n",
    "                or (pair['entity1'] == row['duplicate_id'] and pair['entity2'] == row['original_id'])  # bi-directional match\n",
    "                for pair in identifiers\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Overall matching statistics\n",
    "    total = len(golden_standard)\n",
    "    matched = len(matched_rows)\n",
    "    pct_matched = matched / total * 100\n",
    "\n",
    "    print(f\"Matching Statistics for {match_type}:\")\n",
    "    print(f\"Number of matches in {match_type}:\", len(match_files[match_type]))\n",
    "    print(f\"Total Duplicates in Golden Standard: {total}\")\n",
    "    print(f\"Total Matched Duplicates: {matched}\")\n",
    "    print(f\"Percentage Matched (%): {pct_matched:.2f}\\n\")\n",
    "\n",
    "    # Variation-type analysis\n",
    "    gold_var = golden_standard['variation_type'].value_counts().rename('Golden Standard Count')\n",
    "    match_var = matched_rows['variation_type'].value_counts().rename('Matched Count')\n",
    "\n",
    "    variation_df = pd.concat([gold_var, match_var], axis=1).fillna(0).astype(int)\n",
    "    variation_df['Matched (%)'] = variation_df['Matched Count'] / variation_df['Golden Standard Count'] * 100\n",
    "    variation_df = variation_df.sort_index()\n",
    "\n",
    "    \n",
    "\n",
    "    # Entity-type analysis\n",
    "    gold_ent = golden_standard['entity_type'].value_counts().rename('Golden Standard Count')\n",
    "    match_ent = matched_rows['entity_type'].value_counts().rename('Matched Count')\n",
    "\n",
    "    entity_df = pd.concat([gold_ent, match_ent], axis=1).fillna(0).astype(int)\n",
    "    entity_df['Matched (%)'] = entity_df['Matched Count'] / entity_df['Golden Standard Count'] * 100\n",
    "    entity_df = entity_df.sort_index()\n",
    "\n",
    "    \n",
    "\n",
    "    # Combined variation and entity-type analysis\n",
    "    variation_entity_df = golden_standard.groupby(['variation_type', 'entity_type']).size().unstack(fill_value=0)\n",
    "    matched_variation_entity_df = matched_rows.groupby(['variation_type', 'entity_type']).size().unstack(fill_value=0)\n",
    "\n",
    "    frames = []\n",
    "    for vtype in variation_entity_df.index:\n",
    "        for etype in variation_entity_df.columns:\n",
    "            golden_count = variation_entity_df.at[vtype, etype] if etype in variation_entity_df.columns else 0\n",
    "            matched_count = matched_variation_entity_df.at[vtype, etype] if (vtype in matched_variation_entity_df.index and etype in matched_variation_entity_df.columns) else 0\n",
    "            matched_pct = (matched_count / golden_count * 100) if golden_count > 0 else 0\n",
    "            frames.append({\n",
    "                'variation_type': vtype,\n",
    "                'entity_type': etype,\n",
    "                'Golden Standard Count': golden_count,\n",
    "                'Matched Count': matched_count,\n",
    "                'Matched (%)': matched_pct\n",
    "            })\n",
    "\n",
    "    variation_entity_frame = pd.DataFrame(frames)\n",
    "    variation_entity_frame = variation_entity_frame.sort_values(['variation_type', 'entity_type']).reset_index(drop=True)\n",
    "    variation_frame = variation_entity_frame[variation_entity_frame['Golden Standard Count'] > 1]\n",
    "    variation_frame = variation_frame.sort_values('entity_type').reset_index(drop=True)\n",
    "    variation_frame\n",
    "\n",
    " # Set pandas display options to use the full width of the notebook\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.expand_frame_repr', False)\n",
    "    print(f\"Combined Variation and Entity-Type Analysis for {match_type}:\")\n",
    "    print(variation_frame)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cabd1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16de86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches in Dist: 1506\n",
      "Number of matches in DistLit: 12\n",
      "[{'entity1_id': 'e775f27e-2ec0-42f0-9310-0ac51932263c', 'entity2_id': 'ccc9ccfc-8111-449f-97ea-568c40b0aa75', 'score': 0.5635482668876648}, {'entity1_id': 'a88a96ca-4a8c-4f0a-a9e5-7340fdc60cf4', 'entity2_id': 'd3d90822-0729-4e06-a591-03107ef3782b', 'score': 0.5461329221725464}, {'entity1_id': 'beb2d716-50a2-4f8e-b7ed-8fd685a46efd', 'entity2_id': '3543d36c-da74-4ac4-a443-530c30e5d112', 'score': 0.5020403861999512}, {'entity1_id': '0bd293e3-ee6e-4267-a278-aafc9c62e74b', 'entity2_id': 'cff8cf24-ddc0-4140-b4e9-bfc61b416d8f', 'score': 0.9183053970336914}, {'entity1_id': '8f76781e-2215-44aa-9e5c-fe61f1e5023e', 'entity2_id': '6573d6cc-8f64-4754-bf48-7d8dca63cccc', 'score': 0.7842326164245605}]\n",
      "Number of matched pairs in Dist: 0 / 789 Percentage Matched (%): 0.00\n",
      "[{'entity1_id': '86e280c0-6036-4f9c-a229-7cb2cf00f0a8', 'entity2_id': 'f0af5825-66e5-4e50-8c24-8b75c635a410', 'score': 0.5034458637237549}, {'entity1_id': '10d44b3d-8aca-4410-9c10-126197762b3a', 'entity2_id': '5536945d-f70d-4f38-9d1d-3ec446e93aac', 'score': 0.507127046585083}, {'entity1_id': '4417d108-e256-4d3a-a90c-85a21ac2a40f', 'entity2_id': '7ca2f1ab-3308-4746-a480-5be48664f6a7', 'score': 0.5050807595252991}, {'entity1_id': '5d637363-f92e-4675-a1ae-a2e22ebdb2dc', 'entity2_id': 'e511c128-d619-4c01-accb-998ea28d2b0c', 'score': 0.5135082006454468}, {'entity1_id': '6eb8f2f2-6968-4ce4-8645-11ec8f14e2f9', 'entity2_id': '66b827d7-e4bc-419f-ae22-eab0a1142376', 'score': 0.5033048391342163}]\n",
      "Number of matched pairs in DistLit: 0 / 789 Percentage Matched (%): 0.00\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load your match files\n",
    "match_files_oldform = {\n",
    "    'Dist': json.load(open('Distmatches.json', 'r')),\n",
    "    'DistLit': json.load(open('DistLitmatches.json', 'r'))\n",
    "}\n",
    "\n",
    "print(\"Number of matches in Dist:\", len(match_files_oldform['Dist']))\n",
    "print(\"Number of matches in DistLit:\", len(match_files_oldform['DistLit']))\n",
    "\n",
    "# Loop through the correct dictionary\n",
    "for match_type, data in match_files_oldform.items():\n",
    "    extracted_pairs = []\n",
    "\n",
    "    for item in data:\n",
    "        # Defensive check to ensure keys exist\n",
    "        if all(k in item for k in (\"entity1\", \"entity2\", \"score\")):\n",
    "            extracted_pairs.append({\n",
    "                \"entity1_id\": item[\"entity1\"].rsplit(\"/\", 1)[-1],\n",
    "                \"entity2_id\": item[\"entity2\"].rsplit(\"/\", 1)[-1],\n",
    "                \"score\": item[\"score\"]\n",
    "            })\n",
    "        \n",
    "            \n",
    "    print(extracted_pairs[:5])  # Print first 5 pairs for verification\n",
    "    # Check matches against the golden standard\n",
    "    matched_rows = golden_standard[\n",
    "        golden_standard.apply(\n",
    "            lambda row: any(\n",
    "                (pair['entity1_id'] == row['original_id'] and pair['entity2_id'] == row['duplicate_id']) \n",
    "                for pair in identifiers\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "    ]\n",
    "    pct_matched = len(matched_rows) / len(golden_standard) * 100 if len(golden_standard) > 0 else 0\n",
    "    # Print the number of matched pairs for the current file\n",
    "    print(f\"Number of matched pairs in {match_type}: {len(matched_rows)} / {len(golden_standard)} Percentage Matched (%): {pct_matched:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
