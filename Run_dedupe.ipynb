{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d98ed24",
   "metadata": {},
   "source": [
    "### Dedupe needs more setup therefore a notebook so i can handle it step by Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f4ddc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity type: Person\n",
      "[{'field': 'jobTitle', 'type': 'String'}, {'field': 'name', 'type': 'String'}, {'field': 'birthDate', 'type': 'String'}, {'field': 'email', 'type': 'String'}, {'field': 'gender', 'type': 'String'}, {'field': 'knowsLanguage', 'type': 'String'}]\n",
      "\n",
      "Entity type: Department\n",
      "[{'field': 'name', 'type': 'String'}]\n",
      "\n",
      "Entity type: PostalAddress\n",
      "[{'field': 'streetAddress', 'type': 'String'}, {'field': 'addressLocality', 'type': 'String'}, {'field': 'addressCountry', 'type': 'String'}, {'field': 'postalCode', 'type': 'String'}]\n",
      "\n",
      "Entity type: ContactPoint\n",
      "[{'field': 'contactType', 'type': 'String'}, {'field': 'telephone', 'type': 'String'}, {'field': 'availableLanguage', 'type': 'String'}, {'field': 'email', 'type': 'String'}, {'field': 'faxNumber', 'type': 'String'}]\n",
      "\n",
      "Entity type: MedicalOrganization\n",
      "[{'field': 'name', 'type': 'String'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "\n",
    "ttl_path = \"data/healthcare_graph_Main.ttl\"\n",
    "\n",
    "def extract_dedupe_fields_from_ttl(ttl_path):\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(ttl_path)\n",
    "    \n",
    "    # Map: {entity_type: set([literal_predicate_names])}\n",
    "    type_predicate_map = {}\n",
    "    \n",
    "    for s in set(g.subjects()):\n",
    "        # Get type\n",
    "        types = [str(o) for o in g.objects(s, rdflib.RDF.type)]\n",
    "        if not types:\n",
    "            continue\n",
    "        type_ = types[0].split(\"/\")[-1]  # Or use more sophisticated logic if needed\n",
    "        \n",
    "        # Gather literal predicates\n",
    "        predicates = set()\n",
    "        for p, o in g.predicate_objects(s):\n",
    "            if isinstance(o, rdflib.Literal):\n",
    "                pred_name = p.split(\"/\")[-1] if \"/\" in str(p) else str(p)\n",
    "                predicates.add(pred_name)\n",
    "        if type_ not in type_predicate_map:\n",
    "            type_predicate_map[type_] = set()\n",
    "        type_predicate_map[type_].update(predicates)\n",
    "    \n",
    "    # Build dedupe.io field definitions per entity type\n",
    "    dedupe_fields = {}\n",
    "    for type_, preds in type_predicate_map.items():\n",
    "        dedupe_fields[type_] = [\n",
    "            {'field': pred, 'type': 'String'} for pred in preds if pred.lower() != \"identifier\"\n",
    "]\n",
    "\n",
    "    return dedupe_fields\n",
    "\n",
    "fields_per_type = extract_dedupe_fields_from_ttl(\"data/healthcare_graph_Main.ttl\")\n",
    "for entity_type, fields in fields_per_type.items():\n",
    "    print(f\"Entity type: {entity_type}\")\n",
    "    print(fields)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a7636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modular_methods.graphToText_utils import kg_to_dedupe_dict\n",
    "g = rdflib.Graph() # Main graph\n",
    "g1 = rdflib.Graph() # Replaced low graph\n",
    "g2 = rdflib.Graph() # Train graph\n",
    "g3 = rdflib.Graph() # Replaced medium graph\n",
    "g4 = rdflib.Graph() # Replaced high graph\n",
    "g5 = rdflib.Graph() # Struct low graph\n",
    "g6 = rdflib.Graph() # Struct high graph\n",
    "g7 = rdflib.Graph() # Struct train_low graph\n",
    "g8 = rdflib.Graph() # Struct train_high graph\n",
    "\n",
    "g.parse(\"data/healthcare_graph_Main.ttl\")\n",
    "g1.parse(\"data/healthcare_graph_relation.ttl\")\n",
    "\n",
    "g2.parse(\"data/healthcare_graph_train.ttl\")\n",
    "g3.parse(\"data/healthcare_graph_train_relation.ttl\")\n",
    "g4.parse(\"data/healthcare_graph_replaced_high.ttl\")\n",
    "g5.parse(\"data/healthcare_graph_struct_low.ttl\")\n",
    "g6.parse(\"data/healthcare_graph_struct_high.ttl\")\n",
    "g7.parse(\"data/healthcare_graph_train_struct_low.ttl\")\n",
    "g8.parse(\"data/healthcare_graph_train_struct_high.ttl\")\n",
    "dict_main = kg_to_dedupe_dict(g)\n",
    "\n",
    "train_dict = kg_to_dedupe_dict(g2)\n",
    "dict_relation = kg_to_dedupe_dict(g1)\n",
    "dict_relation_train = kg_to_dedupe_dict(g3)\n",
    "dict_high = kg_to_dedupe_dict(g4)\n",
    "dict_struct_low = kg_to_dedupe_dict(g5)\n",
    "dict_struct_high = kg_to_dedupe_dict(g6)\n",
    "dict_struct_train_low = kg_to_dedupe_dict(g7)\n",
    "dict_struct_train_high = kg_to_dedupe_dict(g8)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872bc3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dedupe data saved to JSON files.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"data/dedupe_data/dict_main.json\", \"w\") as f:\n",
    "    json.dump(dict_main, f, indent=2)\n",
    "with open(\"data/dedupe_data/dict_high.json\", \"w\") as f:\n",
    "    json.dump(dict_high, f, indent=2)\n",
    "with open(\"data/dedupe_data/dict_relation.json\", \"w\") as f:\n",
    "    json.dump(dict_relation, f, indent=2)\n",
    "with open(\"data/dedupe_data/dict_relation_train.json\", \"w\") as f:\n",
    "    json.dump(dict_relation_train, f, indent=2)\n",
    "with open(\"data/dedupe_data/train_dict.json\", \"w\") as f:\n",
    "    json.dump(train_dict, f, indent=2)\n",
    "with open(\"data/dedupe_data/dict_struct_low.json\", \"w\") as f:\n",
    "    json.dump(dict_struct_low, f, indent=2)\n",
    "with open(\"data/dedupe_data/dict_struct_high.json\", \"w\") as f:\n",
    "    json.dump(dict_struct_high, f, indent=2)\n",
    "with open(\"data/dedupe_data/dict_struct_train_low.json\", \"w\") as f:\n",
    "    json.dump(dict_struct_train_low, f, indent=2)\n",
    "with open(\"data/dedupe_data/dict_struct_train_high.json\", \"w\") as f:\n",
    "    json.dump(dict_struct_train_high, f, indent=2)\n",
    "print(\"Dedupe data saved to JSON files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26690cf",
   "metadata": {},
   "source": [
    "## Ground truth for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835c542e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GT files for data/test_golden_standard_high.csv\n",
      "Saved GT files for data/test_golden_standard_struct_low.csv\n",
      "Saved GT files for data/test_golden_standard_struct_high.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# List your test golden standard files here\n",
    "test_gs_files = [\n",
    "    \"data/test_golden_standard_high.csv\",\n",
    "    \"data/test_golden_standard_struct_low.csv\",\n",
    "    \"data/test_golden_standard_struct_high.csv\",\n",
    "    \"data/test_golden_standard_relation.csv\",\n",
    "]\n",
    "\n",
    "output_dir = \"data/dedupe_data/ground_truths_test\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for gs_file in test_gs_files:\n",
    "    golden_standard = pd.read_csv(gs_file)\n",
    "    golden_standard_subset_var = golden_standard[['original_id', 'duplicate_id', 'entity_type','variation_type']]\n",
    "\n",
    "    ground_truth_test = {}\n",
    "\n",
    "    for entity in golden_standard_subset_var['entity_type'].unique():\n",
    "        df_filtered = golden_standard_subset_var[golden_standard_subset_var['entity_type'] == entity]\n",
    "        entity_for_uri = \"Person\" if entity in [\"Person\", \"HealthcarePersonnel\"] else entity\n",
    "        key = \"Person\" if entity in [\"Person\", \"HealthcarePersonnel\"] else entity\n",
    "        if key not in ground_truth_test:\n",
    "            ground_truth_test[key] = []\n",
    "        ground_truth_test[key].extend([\n",
    "            (f\"http://example.org/{entity_for_uri}/{row['original_id']}\", \n",
    "             f\"http://example.org/{entity_for_uri}/{row['duplicate_id']}\")\n",
    "            for _, row in df_filtered.iterrows()\n",
    "        ])\n",
    "    \n",
    "    # Save each GT as a pickle or CSV for later use\n",
    "    base = os.path.splitext(os.path.basename(gs_file))[0]\n",
    "    for entity, pairs in ground_truth_test.items():\n",
    "        # Save as CSV\n",
    "        pd.DataFrame(pairs, columns=[\"uri1\", \"uri2\"]).to_csv(\n",
    "            f\"{output_dir}/{base}_GT_{entity}.csv\", index=False\n",
    "        )\n",
    "        # Optionally, save as pickle for fast loading\n",
    "        # pd.DataFrame(pairs, columns=[\"uri1\", \"uri2\"]).to_pickle(\n",
    "        #     f\"{output_dir}/{base}_GT_{entity}.pkl\"\n",
    "        # )\n",
    "    print(f\"Saved GT files for {gs_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54272207",
   "metadata": {},
   "source": [
    "### Perform the manual labelling on train data NOT on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb06a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GT files for data/train_golden_standard_duplicates_updated.csv\n",
      "Saved GT files for data/train_golden_standard_struct_high.csv\n",
      "Saved GT files for data/train_golden_standard_struct_low.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_gs_files = [\n",
    "    \"data/train_golden_standard_duplicates_updated.csv\",\n",
    "    \"data/train_golden_standard_struct_high.csv\",\n",
    "    \"data/train_golden_standard_struct_low.csv\",\n",
    "    \"data/train_golden_standard_relation.csv\",\n",
    "]\n",
    "\n",
    "output_dir = \"data/dedupe_data/ground_truths_train\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for gs_file in train_gs_files:\n",
    "    golden_standard = pd.read_csv(gs_file)\n",
    "    golden_standard_subset_var = golden_standard[['original_id', 'duplicate_id', 'entity_type','variation_type']]\n",
    "\n",
    "    ground_truth_train = {}\n",
    "\n",
    "    # Mapping to Person for both Person and HealthcarePersonnel\n",
    "    for entity in golden_standard_subset_var['entity_type'].unique():\n",
    "        df_filtered = golden_standard_subset_var[golden_standard_subset_var['entity_type'] == entity]\n",
    "        entity_for_uri = \"Person\" if entity in [\"Person\", \"HealthcarePersonnel\"] else entity\n",
    "        # Use 'Person' as the key for both types\n",
    "        key = \"Person\" if entity in [\"Person\", \"HealthcarePersonnel\"] else entity\n",
    "        if key not in ground_truth_train:\n",
    "            ground_truth_train[key] = []\n",
    "        ground_truth_train[key].extend([\n",
    "            (f\"http://example.org/{entity_for_uri}/{row['original_id']}\", \n",
    "            f\"http://example.org/{entity_for_uri}/{row['duplicate_id']}\")\n",
    "            for _, row in df_filtered.iterrows()\n",
    "        ])\n",
    "\n",
    "    # Save each GT as a pickle or CSV for later use\n",
    "    base = os.path.splitext(os.path.basename(gs_file))[0]\n",
    "    for entity, pairs in ground_truth_test.items():\n",
    "        # Save as CSV\n",
    "        pd.DataFrame(pairs, columns=[\"uri1\", \"uri2\"]).to_csv(\n",
    "            f\"{output_dir}/{base}_GT_{entity}.csv\", index=False\n",
    "        )\n",
    "        # Optionally, save as pickle for fast loading\n",
    "        # pd.DataFrame(pairs, columns=[\"uri1\", \"uri2\"]).to_pickle(\n",
    "        #     f\"{output_dir}/{base}_GT_{entity}.pkl\"\n",
    "        # )\n",
    "    print(f\"Saved GT files for {gs_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# person_variations = golden_standard_subset_var[\n",
    "#     (golden_standard_subset_var['entity_type'] == 'Person') | \n",
    "#     (golden_standard_subset_var['entity_type'] == 'HealthcarePersonnel')\n",
    "# ]\n",
    "\n",
    "# person_variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608bbdec",
   "metadata": {},
   "source": [
    "### Matches per variation type for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "239e03b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person: 60 pairs selected\n",
      "Address: 40 pairs selected\n",
      "HealthcareOrganization: 20 pairs selected\n",
      "ServiceDepartment: 20 pairs selected\n",
      "ContactPoint: 30 pairs selected\n",
      "Person: 10 pairs selected\n",
      "Address: 10 pairs selected\n",
      "HealthcareOrganization: 10 pairs selected\n",
      "ServiceDepartment: 10 pairs selected\n",
      "ContactPoint: 10 pairs selected\n",
      "Person: 10 pairs selected\n",
      "Address: 10 pairs selected\n",
      "HealthcareOrganization: 0 pairs selected\n",
      "ServiceDepartment: 0 pairs selected\n",
      "ContactPoint: 10 pairs selected\n"
     ]
    }
   ],
   "source": [
    "# GT_Address = pd.read_csv('data/dedupe_data/ground_truths_train/train_golden_standard_duplicates_updated_GT_Address.csv')\n",
    "# GT_Person = pd.read_csv('data/dedupe_data/ground_truths_train/train_golden_standard_duplicates_updated_GT_Person.csv')\n",
    "# GT_HCO = pd.read_csv('data/dedupe_data/ground_truths_train/train_golden_standard_duplicates_updated_GT_HealthcareOrganization.csv')\n",
    "# GT_SD = pd.read_csv('data/dedupe_data/ground_truths_train/train_golden_standard_duplicates_updated_GT_ServiceDepartment.csv')\n",
    "# GT_CP = pd.read_csv('data/dedupe_data/ground_truths_train/train_golden_standard_duplicates_updated_GT_ContactPoint.csv')\n",
    "\n",
    "# noise = \"low\"\n",
    "# GT_Address_struct = pd.read_csv(f'data/dedupe_data/ground_truths_train/train_golden_standard_struct_{noise}_GT_Address.csv')\n",
    "# GT_Person_struct = pd.read_csv(f'data/dedupe_data/ground_truths_train/train_golden_standard_struct_{noise}_GT_Person.csv')\n",
    "# GT_HCO_struct = pd.read_csv(f'data/dedupe_data/ground_truths_train/train_golden_standard_struct_{noise}_GT_HealthcareOrganization.csv')\n",
    "# GT_SD_struct = pd.read_csv(f'data/dedupe_data/ground_truths_train/train_golden_standard_struct_{noise}_GT_ServiceDepartment.csv')\n",
    "# GT_CP_struct = pd.read_csv(f'data/dedupe_data/ground_truths_train/train_golden_standard_struct_{noise}_GT_ContactPoint.csv')\n",
    "\n",
    "# Get 7 examples per variation type\n",
    "output_dir = \"data/dedupe_data/ground_truths_per_variation_type\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for file in train_gs_files:\n",
    "    golden_standard = pd.read_csv(file)\n",
    "    golden_standard_subset_var = golden_standard[['original_id', 'duplicate_id', 'entity_type','variation_type']]\n",
    "    \n",
    "\n",
    "    entity_subsets = {\n",
    "        \"Person\": golden_standard_subset_var[\n",
    "            (golden_standard_subset_var['entity_type'] == 'Person') | \n",
    "            (golden_standard_subset_var['entity_type'] == 'HealthcarePersonnel')\n",
    "        ],\n",
    "        \"Address\": golden_standard_subset_var[\n",
    "            golden_standard_subset_var['entity_type'] == 'Address'\n",
    "        ],\n",
    "        \"HealthcareOrganization\": golden_standard_subset_var[\n",
    "            golden_standard_subset_var['entity_type'] == 'HealthcareOrganization'\n",
    "        ],\n",
    "        \"ServiceDepartment\": golden_standard_subset_var[\n",
    "            golden_standard_subset_var['entity_type'] == 'ServiceDepartment'\n",
    "        ],\n",
    "        \"ContactPoint\": golden_standard_subset_var[\n",
    "            golden_standard_subset_var['entity_type'] == 'ContactPoint'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "       # For each entity type, get up to N pairs per variation type\n",
    "    N = 10  # or whatever number you want\n",
    "    gt_by_entity = {}\n",
    "    for entity_type, subset in entity_subsets.items():\n",
    "        gt_by_entity[entity_type] = []\n",
    "        for variation_type in subset['variation_type'].unique():\n",
    "            variation_pairs = subset[subset['variation_type'] == variation_type]\n",
    "            selected_pairs = variation_pairs.head(N)\n",
    "            for _, row in selected_pairs.iterrows():\n",
    "                # Use correct URI prefix for Person/HealthcarePersonnel\n",
    "                entity_for_uri = \"Person\" if entity_type == \"Person\" else entity_type\n",
    "                gt_by_entity[entity_type].append((\n",
    "                    f\"http://example.org/{entity_for_uri}/{row['original_id']}\",\n",
    "                    f\"http://example.org/{entity_for_uri}/{row['duplicate_id']}\"\n",
    "                ))\n",
    "        print(f\"{entity_type}: {len(gt_by_entity[entity_type])} pairs selected\")\n",
    "    file_split = file.split(\"/\")[-1].split(\".\")[0]\n",
    "    # Optionally, save to CSV\n",
    "    for entity_type, pairs in gt_by_entity.items():\n",
    "        pd.DataFrame(pairs, columns=[\"uri1\", \"uri2\"]).to_csv(\n",
    "            f\"{output_dir}/{file_split}_GT_{entity_type}.csv\", index=False)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5778562",
   "metadata": {},
   "source": [
    "### Distinct for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d12a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 768 distinct pairs for Address in train_golden_standard_duplicates_updated\n",
      "Saved 84 distinct pairs for HealthcareOrganization in train_golden_standard_duplicates_updated\n",
      "Saved 684 distinct pairs for ServiceDepartment in train_golden_standard_duplicates_updated\n",
      "Saved 2619 distinct pairs for Person in train_golden_standard_duplicates_updated\n",
      "Saved 768 distinct pairs for ContactPoint in train_golden_standard_duplicates_updated\n",
      "Saved 963 distinct pairs for Address in train_golden_standard_struct_high\n",
      "Saved 963 distinct pairs for ContactPoint in train_golden_standard_struct_high\n",
      "Saved 2739 distinct pairs for Person in train_golden_standard_struct_high\n",
      "Saved 105 distinct pairs for HealthcareOrganization in train_golden_standard_struct_high\n",
      "Saved 858 distinct pairs for ServiceDepartment in train_golden_standard_struct_high\n",
      "Saved 963 distinct pairs for Address in train_golden_standard_struct_low\n",
      "Saved 963 distinct pairs for ContactPoint in train_golden_standard_struct_low\n",
      "Saved 2739 distinct pairs for Person in train_golden_standard_struct_low\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def sample_distinct_pairs(golden_standard_subset, ground_truth_pairs, entity_type, n_neg=3):\n",
    "    \"\"\"Sample distinct pairs for dedupe.io (negatives), avoiding ground truth.\"\"\"\n",
    "    filtered = golden_standard_subset[golden_standard_subset['entity_type'] == entity_type]\n",
    "    all_ids1 = filtered['original_id'].unique()\n",
    "    all_ids2 = filtered['duplicate_id'].unique()\n",
    "    gt_set = set(ground_truth_pairs)\n",
    "    distinct_pairs = set()\n",
    "    attempts = 0\n",
    "    max_attempts = len(gt_set) * n_neg * 10  # Prevent infinite loop\n",
    "    \n",
    "    while len(distinct_pairs) < len(gt_set) * n_neg and attempts < max_attempts:\n",
    "        a, b = random.choice(all_ids1), random.choice(all_ids2)\n",
    "        if (a, b) not in gt_set and (b, a) not in gt_set and (a, b) not in distinct_pairs:\n",
    "            distinct_pairs.add((f\"http://example.org/{entity_type}/{a}\", f\"http://example.org/{entity_type}/{b}\"))\n",
    "        attempts += 1\n",
    "    return list(distinct_pairs)\n",
    "\n",
    "# List your golden files (train or test)\n",
    "golden_files = [\n",
    "    \"data/train_golden_standard_duplicates_updated.csv\",\n",
    "    \"data/train_golden_standard_struct_high.csv\",\n",
    "    \"data/train_golden_standard_struct_low.csv\",\n",
    "    # add test files if needed\n",
    "]\n",
    "\n",
    "output_dir = \"data/dedupe_data/ground_truths_distinct_train\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for file in golden_files:\n",
    "    golden_standard = pd.read_csv(file)\n",
    "    golden_standard_subset = golden_standard[['original_id', 'duplicate_id', 'entity_type','variation_type']]\n",
    "    # Build ground truth for each entity type\n",
    "    ground_truth = {}\n",
    "    for entity in golden_standard_subset['entity_type'].unique():\n",
    "        df_filtered = golden_standard_subset[golden_standard_subset['entity_type'] == entity]\n",
    "        entity_for_uri = \"Person\" if entity in [\"Person\", \"HealthcarePersonnel\"] else entity\n",
    "        key = \"Person\" if entity in [\"Person\", \"HealthcarePersonnel\"] else entity\n",
    "        if key not in ground_truth:\n",
    "            ground_truth[key] = []\n",
    "        ground_truth[key].extend([\n",
    "            (f\"http://example.org/{entity_for_uri}/{row['original_id']}\", \n",
    "             f\"http://example.org/{entity_for_uri}/{row['duplicate_id']}\")\n",
    "            for _, row in df_filtered.iterrows()\n",
    "        ])\n",
    "    # Now sample and save distinct pairs for each entity type\n",
    "    for entity_type in ground_truth.keys():\n",
    "        negatives = sample_distinct_pairs(\n",
    "            golden_standard_subset, ground_truth[entity_type], entity_type, n_neg=3\n",
    "        )\n",
    "        base = os.path.splitext(os.path.basename(file))[0]\n",
    "        pd.DataFrame(negatives, columns=[\"uri1\", \"uri2\"]).to_csv(\n",
    "            f\"{output_dir}/{base}_DISTINCT_{entity_type}.csv\", index=False\n",
    "        )\n",
    "        print(f\"Saved {len(negatives)} distinct pairs for {entity_type} in {base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "525d809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map of entity type to dedupe.io fields\n",
    "ENTITY_FIELDS = {\n",
    "    \"Person\": [\n",
    "        {'field': 'knowsLanguage', 'type': 'String'},\n",
    "        {'field': 'jobTitle', 'type': 'String'},\n",
    "        {'field': 'name', 'type': 'String'},\n",
    "        {'field': 'birthDate', 'type': 'String'},\n",
    "        {'field': 'gender', 'type': 'String'},\n",
    "        {'field': 'email', 'type': 'String'}\n",
    "    ],\n",
    "    \"ContactPoint\": [\n",
    "        {'field': 'faxNumber', 'type': 'String'},\n",
    "        {'field': 'availableLanguage', 'type': 'String'},\n",
    "        {'field': 'telephone', 'type': 'String'},\n",
    "        {'field': 'email', 'type': 'String'},\n",
    "        {'field': 'contactType', 'type': 'String'}\n",
    "    ],\n",
    "    \"Department\": [\n",
    "        {'field': 'name', 'type': 'String'}\n",
    "    ],\n",
    "    \"PostalAddress\": [\n",
    "        {'field': 'addressLocality', 'type': 'String'},\n",
    "        {'field': 'streetAddress', 'type': 'String'},\n",
    "        {'field': 'postalCode', 'type': 'String'},\n",
    "        {'field': 'addressCountry', 'type': 'String'}\n",
    "    ],\n",
    "    \"MedicalOrganization\": [\n",
    "        {'field': 'name', 'type': 'String'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "import dedupe\n",
    "import dedupe.variables\n",
    "\n",
    "ENTITY_FIELDS_dedup= {\"Person\": [\n",
    "        dedupe.variables.String(\"knowsLanguage\"),\n",
    "        dedupe.variables.String(\"jobTitle\"),\n",
    "        dedupe.variables.String(\"name\"),\n",
    "        dedupe.variables.String(\"birthDate\"),\n",
    "        dedupe.variables.String(\"gender\"),\n",
    "        dedupe.variables.String(\"email\")\n",
    "    ],\n",
    "    \"ContactPoint\": [\n",
    "        dedupe.variables.String(\"faxNumber\"),\n",
    "        dedupe.variables.String(\"availableLanguage\"),\n",
    "        dedupe.variables.String(\"telephone\"),\n",
    "        dedupe.variables.String(\"email\"),\n",
    "        dedupe.variables.String(\"contactType\")\n",
    "    ],\n",
    "    \"Department\": [\n",
    "        dedupe.variables.String(\"name\")\n",
    "    ],\n",
    "    \"PostalAddress\": [\n",
    "        dedupe.variables.String(\"addressLocality\"),\n",
    "        dedupe.variables.String(\"streetAddress\"),\n",
    "        dedupe.variables.String(\"postalCode\"),\n",
    "        dedupe.variables.String(\"addressCountry\")\n",
    "    ],\n",
    "    \"MedicalOrganization\": [\n",
    "        dedupe.variables.String(\"name\")\n",
    "    ]}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a364a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_entity(d, entity_type):\n",
    "    search_str = f\"/{entity_type}/\"\n",
    "    return {k: v for k, v in d.items() if search_str in k}\n",
    "\n",
    "\n",
    "person_main = filter_entity(dict_main, \"Person\")\n",
    "person_high = filter_entity(dict_high, \"Person\")\n",
    "person_struct_low = filter_entity(dict_struct_low, \"Person\")\n",
    "person_struct_high = filter_entity(dict_struct_high, \"Person\")\n",
    "train_person_dict1 = filter_entity(train_dict, \"Person\")\n",
    "train_person_low = filter_entity(dict_struct_train_low, \"Person\")\n",
    "train_person_high = filter_entity(dict_struct_train_high, \"Person\")\n",
    "\n",
    "contactpoint_main = filter_entity(dict_main, \"ContactPoint\")\n",
    "contactpoint_high = filter_entity(dict_high, \"ContactPoint\")\n",
    "contactpoint_struct_low = filter_entity(dict_struct_low, \"ContactPoint\")\n",
    "contactpoint_struct_high = filter_entity(dict_struct_high, \"ContactPoint\")\n",
    "train_contactpoint_dict1 = filter_entity(train_dict, \"ContactPoint\")\n",
    "train_contactpoint_low = filter_entity(dict_struct_train_low, \"ContactPoint\")\n",
    "train_contactpoint_high = filter_entity(dict_struct_train_high, \"ContactPoint\")\n",
    "\n",
    "address_main = filter_entity(dict_main, \"Address\")\n",
    "address_high = filter_entity(dict_high, \"Address\")\n",
    "address_struct_low = filter_entity(dict_struct_low, \"Address\")\n",
    "address_struct_high = filter_entity(dict_struct_high, \"Address\")\n",
    "train_address_dict1 = filter_entity(train_dict, \"Address\")\n",
    "train_address_low = filter_entity(dict_struct_train_low, \"Address\")\n",
    "train_address_high = filter_entity(dict_struct_train_high, \"Address\")\n",
    "\n",
    "hco_main = filter_entity(dict_main, \"HealthcareOrganization\")\n",
    "hco_high = filter_entity(dict_high, \"HealthcareOrganization\")\n",
    "hco_struct_low = filter_entity(dict_struct_low, \"HealthcareOrganization\")\n",
    "hco_struct_high = filter_entity(dict_struct_high, \"HealthcareOrganization\")\n",
    "train_hco_dict1 = filter_entity(train_dict, \"HealthcareOrganization\")\n",
    "train_hco_low = filter_entity(dict_struct_train_low, \"HealthcareOrganization\")\n",
    "train_hco_high = filter_entity(dict_struct_train_high, \"HealthcareOrganization\")\n",
    "\n",
    "sd_main = filter_entity(dict_main, \"ServiceDepartment\")\n",
    "sd_high = filter_entity(dict_high, \"ServiceDepartment\")\n",
    "sd_struct_low = filter_entity(dict_struct_low, \"ServiceDepartment\")\n",
    "sd_struct_high = filter_entity(dict_struct_high, \"ServiceDepartment\")\n",
    "train_sd_dict1 = filter_entity(train_dict, \"ServiceDepartment\")\n",
    "train_sd_low = filter_entity(dict_struct_train_low, \"ServiceDepartment\")\n",
    "train_sd_high = filter_entity(dict_struct_train_high, \"ServiceDepartment\")\n",
    "\n",
    "\n",
    "def replace_nan_in_dict(d):\n",
    "    import numpy as np\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            d[k] = replace_nan_in_dict(v)\n",
    "        elif (isinstance(v, float) and np.isnan(v)) or (isinstance(v, str) and v.lower() == 'nan'):\n",
    "            d[k] = ''\n",
    "    return d\n",
    "\n",
    "# Replace NaN values in all dictionaries\n",
    "person_struct_low = replace_nan_in_dict(person_struct_low)\n",
    "person_struct_high = replace_nan_in_dict(person_struct_high)\n",
    "contactpoint_struct_low = replace_nan_in_dict(contactpoint_struct_low)\n",
    "contactpoint_struct_high = replace_nan_in_dict(contactpoint_struct_high)\n",
    "address_struct_low = replace_nan_in_dict(address_struct_low)\n",
    "address_struct_high = replace_nan_in_dict(address_struct_high)\n",
    "hco_struct_low = replace_nan_in_dict(hco_struct_low)\n",
    "hco_struct_high = replace_nan_in_dict(hco_struct_high)\n",
    "sd_struct_low = replace_nan_in_dict(sd_struct_low)\n",
    "sd_struct_high = replace_nan_in_dict(sd_struct_high)\n",
    "train_person_low = replace_nan_in_dict(train_person_low)\n",
    "train_person_high = replace_nan_in_dict(train_person_high)\n",
    "train_contactpoint_low = replace_nan_in_dict(train_contactpoint_low)\n",
    "train_contactpoint_high = replace_nan_in_dict(train_contactpoint_high)\n",
    "train_address_low = replace_nan_in_dict(train_address_low)\n",
    "train_address_high = replace_nan_in_dict(train_address_high)\n",
    "train_hco_low = replace_nan_in_dict(train_hco_low)\n",
    "train_hco_high = replace_nan_in_dict(train_hco_high)\n",
    "train_sd_low = replace_nan_in_dict(train_sd_low)\n",
    "train_sd_high = replace_nan_in_dict(train_sd_high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45483cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'http://example.org/HealthcareOrganization/ba1c6582-628b-41c7-957e-d65369bbfc78': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/6a294d2d-0afe-42dc-b1ad-ea8a8f679cb2': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/6558d20b-b614-48de-a48a-44a3ae383343': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/5eca4e17-0652-4819-b7ea-e133d20e49a5': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/7e4abe74-c3f7-45f6-812b-1e9b4ac50911': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/8697ea93-5041-4718-ac4c-d9ca82b88df0': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/004e0b1f-a990-431d-9252-dc9531866aa1': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/c02c1a44-c971-48bb-9e36-06d3d2a21afe': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/75572bae-67d5-434a-b604-0948bf0deab7': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/4212571c-7d43-4010-b0fc-2246d75a3d57': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/11bba09a-4488-4d58-a39b-a346c6c23b67': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/46c8556e-e0d5-430d-bce3-1f71a2892204': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/2fb59e41-d566-4f4d-969c-5eb1641de08f': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/e3b56436-715c-478d-8d98-41229e5ce08a': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/26f46890-0379-492e-b0fd-1ae0899c0f43': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/fd361bd0-075a-4f05-a64b-626a95245324': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/b9b2ab46-158f-43a1-8c47-884061525764': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/4520ab7c-992a-42fa-b66e-f626af3a58b0': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/32dae1ae-1668-47c4-bded-7783c2586987': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/36779c74-407f-4f44-9fa3-d3a953ac75d1': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/c605b10f-7bde-4db0-b3e2-65a816ca72dd': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/15daf390-76c3-4996-ba1d-88a2226ef15e': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/c38e6e2e-eb33-4863-b655-69c79159be79': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/5009cae7-b883-4d70-b29d-9e230e1d6da9': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/d06b8121-0be8-48a9-9a61-ab3b0bd74fb9': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/c8545b40-126e-4261-99f5-29b5ebc2ebde': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/4eb74531-0a81-4974-9011-7aed47cc25bb': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/338d84bc-ab71-4398-a760-d93d7d10be55': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/62ef6ca0-c03d-45e6-a047-2a86311a3cc1': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/748ff63a-72f6-447d-bc14-f6b76ec3ae3c': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/bae1c1ee-d66c-4a95-8ff8-0907f8e41ad6': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/5ca906b0-7234-40a6-b877-ddb6bb46aee3': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/a2c8cfff-5977-4335-8044-4dd3f2017e49': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/233f3df0-eeac-40c4-be9b-15bb9342a7b9': {'name': ''},\n",
       " 'http://example.org/HealthcareOrganization/4c6eb9ce-d42d-41c3-a665-48381493223a': {'name': ''}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hco_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc43ce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 10 matches and 2739 distinct pairs to data/dedupe_data/labelling_struct/matches_for_labelling_Person_high.csv\n",
      "Wrote 10 matches and 963 distinct pairs to data/dedupe_data/labelling_struct/matches_for_labelling_ContactPoint_high.csv\n",
      "Wrote 10 matches and 963 distinct pairs to data/dedupe_data/labelling_struct/matches_for_labelling_Address_high.csv\n",
      "Wrote 10 matches and 105 distinct pairs to data/dedupe_data/labelling_struct/matches_for_labelling_HealthcareOrganization_high.csv\n",
      "Wrote 10 matches and 858 distinct pairs to data/dedupe_data/labelling_struct/matches_for_labelling_ServiceDepartment_high.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def pairs_to_records(pair_list, dict_main, train_dict):\n",
    "    # Accepts DataFrame or list of tuples\n",
    "    if hasattr(pair_list, \"values\"):  # DataFrame\n",
    "        pair_iter = pair_list[[\"uri1\", \"uri2\"]].values\n",
    "    else:\n",
    "        pair_iter = pair_list\n",
    "    return [\n",
    "        (dict_main[a], train_dict[b])\n",
    "        for a, b in pair_iter\n",
    "        if a in dict_main and b in train_dict\n",
    "    ]\n",
    "\n",
    "\n",
    "def write_pairs_to_csv(pairs, filename, expected_fields=None):\n",
    "    \"\"\"Write pairs of records to a CSV file for manual editing.\"\"\"\n",
    "    if not pairs:\n",
    "        print(\"No pairs to write.\")\n",
    "        return\n",
    "\n",
    "    # If not provided, infer from all pairs\n",
    "    if expected_fields is None:\n",
    "        all_keys_left = set()\n",
    "        all_keys_right = set()\n",
    "        for rec1, rec2 in pairs:\n",
    "            all_keys_left.update(rec1.keys())\n",
    "            all_keys_right.update(rec2.keys())\n",
    "        expected_fields = sorted(all_keys_left | all_keys_right)  # union\n",
    "\n",
    "    fieldnames = ['label'] + [f'left_{k}' for k in expected_fields] + [f'right_{k}' for k in expected_fields]\n",
    "\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for rec1, rec2 in pairs:\n",
    "            row = {'label': ''}\n",
    "            for k in expected_fields:\n",
    "                row[f\"left_{k}\"] = rec1.get(k, '')\n",
    "                row[f\"right_{k}\"] = rec2.get(k, '')\n",
    "            writer.writerow(row)\n",
    "\n",
    "def read_pairs_from_csv(filename):\n",
    "    \"\"\"Read pairs of records from a CSV file (after manual annotation).\"\"\"\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            rec1 = {k[5:]: v for k, v in row.items() if k.startswith('left_')}\n",
    "            rec2 = {k[6:]: v for k, v in row.items() if k.startswith('right_')}\n",
    "            label = row.get('label', '')\n",
    "            pairs.append((rec1, rec2))\n",
    "            labels.append(label)\n",
    "    return pairs, labels\n",
    "\n",
    "GT_Person = pd.read_csv('data/dedupe_data/ground_truths_per_variation_type/train_golden_standard_duplicates_updated_GT_Person.csv')\n",
    "GT_Address = pd.read_csv('data/dedupe_data/ground_truths_per_variation_type/train_golden_standard_duplicates_updated_GT_Address.csv')\n",
    "GT_HCO = pd.read_csv('data/dedupe_data/ground_truths_per_variation_type/train_golden_standard_duplicates_updated_GT_HealthcareOrganization.csv')\n",
    "GT_SD = pd.read_csv('data/dedupe_data/ground_truths_per_variation_type/train_golden_standard_duplicates_updated_GT_ServiceDepartment.csv')\n",
    "GT_CP = pd.read_csv('data/dedupe_data/ground_truths_per_variation_type/train_golden_standard_duplicates_updated_GT_ContactPoint.csv')\n",
    "\n",
    "DP_Person = pd.read_csv('data/dedupe_data/ground_truths_distinct_train/train_golden_standard_duplicates_updated_DISTINCT_Person.csv')\n",
    "DP_Address = pd.read_csv('data/dedupe_data/ground_truths_distinct_train/train_golden_standard_duplicates_updated_DISTINCT_Address.csv')\n",
    "DP_HCO = pd.read_csv('data/dedupe_data/ground_truths_distinct_train/train_golden_standard_duplicates_updated_DISTINCT_HealthcareOrganization.csv')\n",
    "DP_SD = pd.read_csv('data/dedupe_data/ground_truths_distinct_train/train_golden_standard_duplicates_updated_DISTINCT_ServiceDepartment.csv')\n",
    "DP_CP = pd.read_csv('data/dedupe_data/ground_truths_distinct_train/train_golden_standard_duplicates_updated_DISTINCT_ContactPoint.csv')\n",
    "\n",
    "noise = \"high\"\n",
    "if noise == \"low\":\n",
    "    train_dict = dict_struct_train_low\n",
    "    GT_Person = pd.read_csv(f'data/dedupe_data/ground_truths_per_variation_type/train_golden_standard_struct_{noise}_GT_Person.csv')\n",
    "    GT_Address = pd.read_csv(f'data/dedupe_data/ground_truths_per_variation_type/train_golden_standard_struct_{noise}_GT_Address.csv')\n",
    "    GT_CP = pd.read_csv(f'data/dedupe_data/ground_truths_per_variation_type/train_golden_standard_struct_{noise}_GT_ContactPoint.csv')\n",
    "    DP_Person = pd.read_csv(f'data/dedupe_data/ground_truths_distinct_train/train_golden_standard_struct_{noise}_DISTINCT_Person.csv')\n",
    "    DP_Address = pd.read_csv(f'data/dedupe_data/ground_truths_distinct_train/train_golden_standard_struct_{noise}_DISTINCT_Address.csv')\n",
    "    DP_CP = pd.read_csv(f'data/dedupe_data/ground_truths_distinct_train/train_golden_standard_struct_{noise}_DISTINCT_ContactPoint.csv')\n",
    "\n",
    "    matches_Person = pairs_to_records(GT_Person, dict_main, dict_struct_train_low)\n",
    "    distinct_Person = pairs_to_records(DP_Person, dict_main, dict_struct_train_low)\n",
    "\n",
    "    matches_ContactPoint = pairs_to_records(GT_CP, dict_main, dict_struct_train_low)\n",
    "    distinct_ContactPoint = pairs_to_records(DP_CP, dict_main, dict_struct_train_low)\n",
    "\n",
    "    matches_Address = pairs_to_records(GT_Address, dict_main, dict_struct_train_low)\n",
    "    distinct_Address = pairs_to_records(DP_Address, dict_main, dict_struct_train_low)\n",
    "\n",
    "        #Example usage:\n",
    "    for matches , distinct, entity_type in [\n",
    "        (matches_Person, distinct_Person, \"Person\"),\n",
    "        (matches_ContactPoint, distinct_ContactPoint, \"ContactPoint\"),\n",
    "        (matches_Address, distinct_Address, \"Address\")\n",
    "    ]:\n",
    "        filename = f\"data/dedupe_data/labelling_struct/matches_for_labelling_{entity_type}_{noise}.csv\"\n",
    "        write_pairs_to_csv(matches, filename)\n",
    "        write_pairs_to_csv(distinct, filename.replace(\"matches_for_labelling\", \"distinct_pairs_for_labelling\"))\n",
    "        print(f\"Wrote {len(matches)} matches and {len(distinct)} distinct pairs to {filename}\")\n",
    "    \n",
    "\n",
    "if noise == \"high\":\n",
    "    train_dict = dict_struct_train_high\n",
    "    GT_Person = pd.read_csv(f'data/dedupe_data/ground_truths_per_variation_type/train_golden_standard_struct_{noise}_GT_Person.csv')\n",
    "    GT_Address = pd.read_csv(f'data/dedupe_data/ground_truths_per_variation_type/train_golden_standard_struct_{noise}_GT_Address.csv')\n",
    "    GT_CP = pd.read_csv(f'data/dedupe_data/ground_truths_per_variation_type/train_golden_standard_struct_{noise}_GT_ContactPoint.csv')\n",
    "    GT_HCO = pd.read_csv(f'data/dedupe_data/ground_truths_per_variation_type/train_golden_standard_struct_{noise}_GT_HealthcareOrganization.csv')\n",
    "    GT_SD = pd.read_csv(f'data/dedupe_data/ground_truths_per_variation_type/train_golden_standard_struct_{noise}_GT_ServiceDepartment.csv')\n",
    "    DP_Person = pd.read_csv(f'data/dedupe_data/ground_truths_distinct_train/train_golden_standard_struct_{noise}_DISTINCT_Person.csv')\n",
    "    DP_Address = pd.read_csv(f'data/dedupe_data/ground_truths_distinct_train/train_golden_standard_struct_{noise}_DISTINCT_Address.csv')\n",
    "    DP_CP = pd.read_csv(f'data/dedupe_data/ground_truths_distinct_train/train_golden_standard_struct_{noise}_DISTINCT_ContactPoint.csv')\n",
    "    DP_HCO = pd.read_csv(f'data/dedupe_data/ground_truths_distinct_train/train_golden_standard_struct_{noise}_DISTINCT_HealthcareOrganization.csv')\n",
    "    DP_SD = pd.read_csv(f'data/dedupe_data/ground_truths_distinct_train/train_golden_standard_struct_{noise}_DISTINCT_ServiceDepartment.csv')\n",
    "\n",
    "    matches_Person = pairs_to_records(GT_Person, dict_main, dict_struct_train_high)\n",
    "    distinct_Person = pairs_to_records(DP_Person, dict_main, dict_struct_train_high)\n",
    "\n",
    "    matches_ContactPoint = pairs_to_records(GT_CP, dict_main, dict_struct_train_high)\n",
    "    distinct_ContactPoint = pairs_to_records(DP_CP, dict_main, dict_struct_train_high)\n",
    "\n",
    "    matches_Address = pairs_to_records(GT_Address, dict_main, dict_struct_train_high)\n",
    "    distinct_Address = pairs_to_records(DP_Address, dict_main, dict_struct_train_high)\n",
    "\n",
    "    matches_HCO = pairs_to_records(GT_HCO, dict_main, dict_struct_train_high)\n",
    "    distinct_HCO = pairs_to_records(DP_HCO, dict_main, dict_struct_train_high)\n",
    "\n",
    "    matches_SD = pairs_to_records(GT_SD, dict_main, dict_struct_train_high)\n",
    "    distinct_SD = pairs_to_records(DP_SD, dict_main, dict_struct_train_high)\n",
    "\n",
    "        #Example usage:\n",
    "    for matches , distinct, entity_type in [\n",
    "        (matches_Person, distinct_Person, \"Person\"),\n",
    "        (matches_ContactPoint, distinct_ContactPoint, \"ContactPoint\"),\n",
    "        (matches_Address, distinct_Address, \"Address\"),\n",
    "        (matches_HCO, distinct_HCO, \"HealthcareOrganization\"),\n",
    "        (matches_SD, distinct_SD, \"ServiceDepartment\")\n",
    "    ]:\n",
    "        filename = f\"data/dedupe_data/labelling_struct/matches_for_labelling_{entity_type}_{noise}.csv\"\n",
    "        write_pairs_to_csv(matches, filename)\n",
    "        write_pairs_to_csv(distinct, filename.replace(\"matches_for_labelling\", \"distinct_pairs_for_labelling\"))\n",
    "        print(f\"Wrote {len(matches)} matches and {len(distinct)} distinct pairs to {filename}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# matches_Person = pairs_to_records(GT_Person, dict_main, train_dict)\n",
    "# distinct_Person = pairs_to_records(DP_Person, dict_main, train_dict)\n",
    "\n",
    "# matches_ContactPoint = pairs_to_records(GT_CP, dict_main, train_dict)\n",
    "# distinct_ContactPoint = pairs_to_records(DP_CP, dict_main, train_dict)\n",
    "\n",
    "# matches_Address = pairs_to_records(GT_Address, dict_main, train_dict)\n",
    "# distinct_Address = pairs_to_records(DP_Address, dict_main, train_dict)\n",
    "\n",
    "# matches_HCO = pairs_to_records(GT_HCO, dict_main, train_dict)\n",
    "# distinct_HCO = pairs_to_records(DP_HCO, dict_main, train_dict)\n",
    "\n",
    "# matches_SD = pairs_to_records(GT_SD, dict_main, train_dict)\n",
    "# distinct_SD = pairs_to_records(DP_SD, dict_main, train_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #Example usage:\n",
    "# for matches , distinct, entity_type in [\n",
    "#     (matches_Person, distinct_Person, \"Person\"),\n",
    "#     (matches_ContactPoint, distinct_ContactPoint, \"ContactPoint\"),\n",
    "#     (matches_Address, distinct_Address, \"Address\"),\n",
    "#     (matches_HCO, distinct_HCO, \"HealthcareOrganization\"),\n",
    "#     (matches_SD, distinct_SD, \"ServiceDepartment\")\n",
    "# ]:\n",
    "#     filename = f\"data/dedupe_data/labelling_train/matches_for_labelling_{entity_type}.csv\"\n",
    "#     write_pairs_to_csv(matches, filename)\n",
    "#     write_pairs_to_csv(distinct, filename.replace(\"matches_for_labelling\", \"distinct_pairs_for_labelling\"))\n",
    "#     print(f\"Wrote {len(matches)} matches and {len(distinct)} distinct pairs to {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
